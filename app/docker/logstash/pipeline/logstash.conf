# Pipeline-level settings for better memory management
# These settings are overridden by environment variables in docker-compose.yml
# but can be set here as defaults

input {
  beats {
    port => 5044
  }
  file {
    path => "/logs/postgresql.log"
    start_position => "end"  # Only read new lines, not entire file
    sincedb_path => "/usr/share/logstash/.sincedb_postgresql"  # Track position
    type => "postgresql"
    codec => "plain"
    # Ignore old/rotated files
    ignore_older => 86400  # Ignore files older than 24 hours
  }
  file {
    path => "/app/logs/frontend_access.log"
    start_position => "end"  # Only read new lines
    sincedb_path => "/usr/share/logstash/.sincedb_security"  # Track position
    type => "security"
    codec => "plain"
    ignore_older => 604800  # Ignore files older than 7 days
    stat_interval => 1  # Check for file changes every 1 second (default is 1 second, but explicit is better)
    discover_interval => 5  # Check for new files every 5 seconds
  }
  
  # JDBC input for audit_log table (trigger-based logging)
  # Note: Requires PostgreSQL JDBC driver in /usr/share/logstash/vendor/jdbc/
  # Download from: https://jdbc.postgresql.org/download/
  jdbc {
    jdbc_connection_string => "jdbc:postgresql://postgres:5432/thesisdb"
    jdbc_user => "myuser"
    jdbc_password => "mypassword"
    jdbc_driver_library => "/usr/share/logstash/vendor/jdbc/postgresql-42.7.1.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    jdbc_paging_enabled => true
    jdbc_page_size => 500  # Reduced from 1000 to 500 to lower memory usage
    statement => "SELECT id, table_name, operation, old_data::text as old_data, new_data::text as new_data, changed_fields::text as changed_fields, user_name, application_name, client_address::text as client_address, transaction_id, timestamp, row_id FROM audit_log WHERE id > :sql_last_value ORDER BY id ASC"
    schedule => "*/5 * * * *"  # Run every 5 minutes
    type => "audit_log"
    use_column_value => true
    tracking_column => "id"
    tracking_column_type => "numeric"
    last_run_metadata_path => "/usr/share/logstash/.logstash_jdbc_last_run_audit"
    clean_run => false
    jdbc_fetch_size => 500  # Limit fetch size to reduce memory
  }
}

filter {
  # Skip empty messages
  if [message] == "" {
    drop { }
  }
  
  # Drop routine PostgreSQL logs that aren't useful (major performance improvement)
  # Do this early to reduce memory usage
  if [type] == "postgresql" {
    # Drop checkpoint logs (very frequent and not useful)
    if [message] =~ /LOG:\s+checkpoint (starting|complete)/ {
      drop { }
    }
    # Drop routine connection/disconnection logs (too verbose, but keep errors)
    if [message] =~ /LOG:\s+(connection received|connection authorized|disconnection: session time)/ {
      drop { }
    }
    # Only drop routine transaction logs, not errors
    if [message] =~ /LOG:\s+(WAL|transaction log)/ and [message] !~ /ERROR/ {
      drop { }
    }
    # Drop statement logs that don't contain useful information (reduce volume significantly)
    if [message] =~ /LOG:\s+statement: (BEGIN|COMMIT|ROLLBACK|SET|SHOW)/ {
      drop { }
    }
  }
  
  # Parse security logs (FRONTEND_ACCESS, UNAUTHORIZED_ACCESS, FAKE_DATA_ACCESS, REAL_DATA_ACCESS)
  if [type] == "security" {
    # Parse FRONTEND_ACCESS logs - add tag first, then parse
    if [message] =~ /FRONTEND_ACCESS/ {
      # Add tag immediately when FRONTEND_ACCESS is detected
      mutate {
        add_tag => ["frontend_access"]
      }
      
      grok {
        match => {
          "message" => [
            "%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME}.*FRONTEND_ACCESS - accessed_row:%{INT:accessed_row} authorized_rows:%{GREEDYDATA:authorized_rows} is_authorized:%{WORD:is_authorized} requested_by:%{DATA:user} timestamp:%{TIMESTAMP_ISO8601:timestamp}",
            "%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME}.*FRONTEND_ACCESS - accessing_rows:%{GREEDYDATA:accessing_rows} requested_by:%{DATA:user} timestamp:%{TIMESTAMP_ISO8601:timestamp}",
            ".*FRONTEND_ACCESS - accessed_row:%{INT:accessed_row} authorized_rows:%{GREEDYDATA:authorized_rows} is_authorized:%{WORD:is_authorized} requested_by:%{DATA:user} timestamp:%{TIMESTAMP_ISO8601:timestamp}.*",
            ".*FRONTEND_ACCESS - accessing_rows:%{GREEDYDATA:accessing_rows} requested_by:%{DATA:user} timestamp:%{TIMESTAMP_ISO8601:timestamp}.*"
          ]
        }
        tag_on_failure => []
      }
      
      # Rename user to requested_by for consistency (only if user field exists)
      if [user] {
        mutate {
          rename => { "user" => "requested_by" }
        }
      }
      
      # If accessed_row is present and is_authorized is false, add unauthorized tag
      if [accessed_row] and [is_authorized] == "False" {
        mutate {
          add_tag => ["unauthorized_access"]
        }
      }
    } else if [message] =~ /UNAUTHORIZED_ACCESS/ {
      grok {
        match => {
          "message" => ".*UNAUTHORIZED_ACCESS - row_number:%{INT:unauthorized_row} .* authorized_rows:%{DATA:authorized_rows} requested_by:%{DATA:user} timestamp:%{TIMESTAMP_ISO8601:timestamp}.*"
        }
      }
      mutate {
        add_tag => ["unauthorized_access"]
        # Rename user to requested_by for consistency
        rename => { "user" => "requested_by" }
      }
    } else if [message] =~ /FAKE_DATA_ACCESS/ {
      grok {
        match => {
          "message" => ".*FAKE_DATA_ACCESS - row_number:%{INT:fake_row} .* is_fake_row:%{WORD:is_fake_row} authorized_rows:%{GREEDYDATA:authorized_rows} requested_by:%{DATA:user} timestamp:%{TIMESTAMP_ISO8601:timestamp}.*"
        }
      }
      mutate {
        add_tag => ["fake_data_access", "data_access"]
        rename => { 
          "fake_row" => "row_number"
          "user" => "requested_by"
        }
      }
    } else if [message] =~ /REAL_DATA_ACCESS/ {
      grok {
        match => {
          "message" => ".*REAL_DATA_ACCESS - row_number:%{INT:real_row} .* is_fake_row:%{WORD:is_fake_row} authorized_rows:%{GREEDYDATA:authorized_rows} requested_by:%{DATA:user} timestamp:%{TIMESTAMP_ISO8601:timestamp}.*"
        }
      }
      mutate {
        add_tag => ["real_data_access", "data_access"]
        rename => { 
          "real_row" => "row_number"
          "user" => "requested_by"
        }
      }
    }
    
    # Remove message field after parsing to save space (structured fields are already extracted)
    mutate {
      remove_field => [ "message" ]
    }
  }
  
  # Parse PostgreSQL logs
  if [type] == "postgresql" {
           # Try to parse structured logs first
           grok {
             match => {
               "message" => [
                 "%{TIMESTAMP_ISO8601:timestamp} UTC \[%{POSINT:pid}\] user=%{DATA:user},db=%{DATA:db},app=%{DATA:app},client=%{IP:client} %{GREEDYDATA:log_message}",
                 "%{TIMESTAMP_ISO8601:timestamp} UTC \[%{POSINT:pid}\] %{GREEDYDATA:log_message}",
                 "%{TIMESTAMP_ISO8601:timestamp} GMT \[%{POSINT:pid}\]: \[%{DATA:session}\] user=%{DATA:user},db=%{DATA:db},app=%{DATA:app},client=%{IP:client} %{GREEDYDATA:log_message}",
                 "%{TIMESTAMP_ISO8601:timestamp} GMT \[%{POSINT:pid}\]: \[%{DATA:session}\] user=%{DATA:user},db=%{DATA:db},app=%{DATA:app},client=%{IP:client} %{GREEDYDATA:log_message}",
                 "%{TIMESTAMP_ISO8601:timestamp} GMT \[%{POSINT:pid}\] user=%{DATA:user},db=%{DATA:db},app=%{DATA:app},client=%{IP:client} %{GREEDYDATA:log_message}",
                 "%{TIMESTAMP_ISO8601:timestamp} GMT \[%{POSINT:pid}\] %{GREEDYDATA:log_message}"
               ]
             }
             tag_on_failure => []
           }

           # Convert timestamp to proper format for Elasticsearch
           if [timestamp] {
             date {
               match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
               target => "@timestamp"
             }
             # Remove the original timestamp field to avoid conflicts
             mutate {
               remove_field => [ "timestamp" ]
             }
           }
    
    # For SQL fragments, don't add failure tag - they're valid fragments
    if "_grokparsefailure" in [tags] {
      mutate {
        remove_tag => ["_grokparsefailure"]
        add_tag => ["sql_fragment"]
      }
    }
    
    # Add a tag to identify different log types
    if [log_message] {
      mutate {
        add_tag => ["postgres_log"]
      }
    }
    if [sql_fragment] {
      mutate {
        add_tag => ["sql_fragment"]
      }
    }
    
    # Detect queries on tables from backend/models.py (direct database access)
    # Tables: test1123, Users
    # Check for test1123 table
    if "test1123" in [message] {
      mutate {
        add_tag => ["database_access"]
        add_field => { "table" => "test1123" }
      }
      
      # Try to extract row numbers from WHERE clauses or SELECT queries
      if "row_number" in [message] {
        # First try on message field
        grok {
          match => {
            "message" => ".*row_number\s*=\s*(?<queried_row>\d+)"
          }
        }
        
        # Also try to capture rows from IN clauses
        if ![queried_row] {
          grok {
            match => {
              "message" => ".*row_number\s*IN\s*\((?<queried_rows>[0-9\s,]+)\)"
            }
          }
        }
        
        mutate {
          add_tag => ["row_query"]
        }
        
        if [queried_row] {
          mutate {
            rename => { "queried_row" => "accessed_row" }
          }
        }
      }
      
      # Extract username if available
      if [user] {
        mutate {
          add_field => { "requested_by" => "%{user}" }
        }
      }
    }
    # Check for test1123 in log_message
    else if "test1123" in [log_message] {
      mutate {
        add_tag => ["database_access"]
        add_field => { "table" => "test1123" }
      }
      
      # Try to extract row numbers from WHERE clauses or SELECT queries
      if "row_number" in [log_message] {
        grok {
          match => {
            "log_message" => ".*row_number\s*=\s*(?<queried_row>\d+)"
          }
        }
        
        if ![queried_row] {
          grok {
            match => {
              "log_message" => ".*row_number\s*IN\s*\((?<queried_rows>[0-9\s,]+)\)"
            }
          }
        }
        
        mutate {
          add_tag => ["row_query"]
        }
        
        if [queried_row] {
          mutate {
            rename => { "queried_row" => "accessed_row" }
          }
        }
      }
      
      # Extract username if available
      if [user] {
        mutate {
          add_field => { "requested_by" => "%{user}" }
        }
      }
    }
    # Check for Users table
    else if "Users" in [message] {
      mutate {
        add_tag => ["database_access"]
        add_field => { "table" => "Users" }
      }
      
      # Extract username if available
      if [user] {
        mutate {
          add_field => { "requested_by" => "%{user}" }
        }
      }
    }
    else if "Users" in [log_message] {
      mutate {
        add_tag => ["database_access"]
        add_field => { "table" => "Users" }
      }
      
      # Extract username if available
      if [user] {
        mutate {
          add_field => { "requested_by" => "%{user}" }
        }
      }
    }
  }
  
  # Clean up _grokparsefailure tags for security-related logs
  # (We don't want security logs with parsing errors)
  if ("frontend_access" in [tags] or "database_access" in [tags] or "unauthorized_access" in [tags]) and "_grokparsefailure" in [tags] {
    mutate {
      remove_tag => ["_grokparsefailure"]
    }
  }
  
  # Remove message and user fields from security logs before output
  if "frontend_access" in [tags] or "database_access" in [tags] or "unauthorized_access" in [tags] or "fake_data_access" in [tags] or "real_data_access" in [tags] {
    mutate {
      remove_field => [ "message", "user" ]
    }
  }
  
  # Filter for audit log entries from trigger function
  if [type] == "audit_log" {
    mutate {
      add_tag => ["database_audit", "trigger_log"]
      rename => {
        "table_name" => "audit_table"
        "operation" => "audit_operation"
        "user_name" => "audit_user"
        "application_name" => "audit_app"
        "client_address" => "audit_client"
        "transaction_id" => "audit_transaction_id"
        "row_id" => "audit_row_id"
      }
    }
    
    # Parse timestamp field (PostgreSQL returns timestamp in various formats)
    if [timestamp] {
      # Try multiple timestamp formats
      date {
        match => [ 
          "timestamp", 
          "yyyy-MM-dd HH:mm:ss.SSSSSS",
          "yyyy-MM-dd HH:mm:ss.SSS",
          "yyyy-MM-dd HH:mm:ss",
          "ISO8601"
        ]
        target => "@timestamp"
      }
      # If date parsing fails, use current time but keep original timestamp
      if "_dateparsefailure" in [tags] {
        mutate {
          remove_tag => ["_dateparsefailure"]
          add_field => { "timestamp_original" => "%{timestamp}" }
        }
      }
    }
    
    # Add flags for data presence
    if [old_data] {
      mutate {
        add_field => { "has_old_data" => "true" }
      }
    }
    
    if [new_data] {
      mutate {
        add_field => { "has_new_data" => "true" }
      }
    }
    
    # Add specific tags based on operation type
    if [audit_operation] == "INSERT" {
      mutate {
        add_tag => ["insert_operation"]
      }
    } else if [audit_operation] == "UPDATE" {
      mutate {
        add_tag => ["update_operation"]
      }
    } else if [audit_operation] == "DELETE" {
      mutate {
        add_tag => ["delete_operation"]
      }
    }
  }
}

output {
  # Output audit logs from triggers to dedicated index
  if "database_audit" in [tags] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "postgres-audit-logs-%{+YYYY.MM.dd}"
      # Batching is controlled by pipeline-level settings (PIPELINE_BATCH_SIZE, PIPELINE_BATCH_DELAY)
      # Use bulk API for better performance
      action => "index"
    }
  }
  # Output security logs
  else if "frontend_access" in [tags] or "database_access" in [tags] or "unauthorized_access" in [tags] or "fake_data_access" in [tags] or "real_data_access" in [tags] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "security-logs-%{+YYYY.MM.dd}"
      # Batching is controlled by pipeline-level settings (PIPELINE_BATCH_SIZE, PIPELINE_BATCH_DELAY)
      action => "index"
    }
  } 
  # Output other PostgreSQL logs (only important ones after filtering)
  else {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "postgres-logs-%{+YYYY.MM.dd}"
      # Batching is controlled by pipeline-level settings (PIPELINE_BATCH_SIZE, PIPELINE_BATCH_DELAY)
      action => "index"
    }
  }
  # Removed stdout output - it was causing massive I/O overhead
  # Uncomment below for debugging only:
  # stdout { codec => rubydebug }
}
