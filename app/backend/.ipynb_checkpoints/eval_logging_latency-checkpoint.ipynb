{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging latency evaluation (Postgres → Elasticsearch alerts-index)\n",
    "\n",
    "This notebook:\n",
    "- Connects to Postgres directly.\n",
    "- Runs test queries (you can edit them).\n",
    "- Records wall-clock timestamps around each query.\n",
    "- Polls Elasticsearch `alerts-index` and records the newest alert `@timestamp` observed after each query.\n",
    "\n",
    "**Assumptions (defaults from `app/docker/docker-compose.yml`):**\n",
    "- Postgres: `localhost:5432`, db `thesisdb`, user `myuser`, password `mypassword`\n",
    "- Elasticsearch: `http://localhost:9200` (security disabled)\n",
    "\n",
    "You will manually map queries ↔ alerts, using the recorded time windows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import requests\n",
    "\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def parse_es_ts(ts: Optional[str]) -> Optional[datetime]:\n",
    "    if not ts:\n",
    "        return None\n",
    "    # Elasticsearch usually returns RFC3339 / ISO8601 with Z\n",
    "    # Example: 2025-12-04T05:45:00.968013336Z\n",
    "    s = ts\n",
    "    if s.endswith('Z'):\n",
    "        s = s[:-1] + '+00:00'\n",
    "    # Python can't parse > 6 fractional digits; trim if needed\n",
    "    if '.' in s:\n",
    "        head, frac = s.split('.', 1)\n",
    "        if '+' in frac:\n",
    "            frac_digits, tz = frac.split('+', 1)\n",
    "            frac_digits = frac_digits[:6]\n",
    "            s = f\"{head}.{frac_digits}+{tz}\"\n",
    "    return datetime.fromisoformat(s)\n",
    "\n",
    "\n",
    "PG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"thesisdb\",\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "}\n",
    "\n",
    "ES = {\n",
    "    \"base_url\": \"http://localhost:9200\",\n",
    "    \"alerts_index\": \"alerts-index\",\n",
    "}\n",
    "\n",
    "print(\"Configured PG:\", {k: v for k, v in PG.items() if k != \"password\"})\n",
    "print(\"Configured ES:\", ES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_connect():\n",
    "    conn = psycopg2.connect(**PG)\n",
    "    conn.autocommit = True\n",
    "    return conn\n",
    "\n",
    "\n",
    "def es_latest_alert_ts() -> Optional[str]:\n",
    "    \"\"\"Return newest alerts-index @timestamp (string) or None if empty/not found.\"\"\"\n",
    "    url = f\"{ES['base_url'].rstrip('/')}/{ES['alerts_index']}/_search\"\n",
    "    body = {\n",
    "        \"size\": 1,\n",
    "        \"sort\": [{\"@timestamp\": \"desc\"}],\n",
    "        \"_source\": [\"@timestamp\", \"rule_id\", \"message\", \"alert_id\"],\n",
    "        \"query\": {\"match_all\": {}},\n",
    "    }\n",
    "    r = requests.get(url, json=body, timeout=10)\n",
    "    if r.status_code == 404:\n",
    "        return None\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    hits = data.get(\"hits\", {}).get(\"hits\", [])\n",
    "    if not hits:\n",
    "        return None\n",
    "    return hits[0].get(\"_source\", {}).get(\"@timestamp\")\n",
    "\n",
    "\n",
    "def es_wait_new_alert(after_ts: Optional[str], timeout_s: float = 30.0, poll_s: float = 1.0) -> Dict[str, Any]:\n",
    "    \"\"\"Poll until alerts-index newest @timestamp changes (or timeout).\"\"\"\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        cur = es_latest_alert_ts()\n",
    "        if cur and cur != after_ts:\n",
    "            return {\"status\": \"updated\", \"newest_alert_ts\": cur, \"wait_s\": round(time.time() - start, 3)}\n",
    "        if time.time() - start > timeout_s:\n",
    "            return {\"status\": \"timeout\", \"newest_alert_ts\": cur, \"wait_s\": round(time.time() - start, 3)}\n",
    "        time.sleep(poll_s)\n",
    "\n",
    "\n",
    "# Smoke tests\n",
    "try:\n",
    "    with pg_connect() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT 1\")\n",
    "            print(\"Postgres OK:\", cur.fetchone())\n",
    "except Exception as e:\n",
    "    print(\"Postgres connect FAILED:\", repr(e))\n",
    "\n",
    "try:\n",
    "    print(\"alerts-index newest @timestamp:\", es_latest_alert_ts())\n",
    "except Exception as e:\n",
    "    print(\"Elasticsearch query FAILED:\", repr(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your test queries here.\n",
    "# Tip: use SET application_name so your trigger/audit pipeline can attribute these actions.\n",
    "# NOTE: If your audit trigger only logs DML, SELECT won't create audit rows.\n",
    "\n",
    "TEST_QUERIES: List[Dict[str, Any]] = [\n",
    "    {\n",
    "        \"name\": \"select_count\",\n",
    "        \"sql\": \"SET application_name = 'eval_logging'; SELECT count(*) FROM test1123_f;\",\n",
    "    }#,\n",
    "    #{\n",
    "    #    \"name\": \"delete_one_row\",\n",
    "        # adjust predicate/PK to your schema\n",
    "    #    \"sql\": \"SET application_name = 'eval_logging'; DELETE FROM test1123_f WHERE row_number = 9999;\",\n",
    "    #},\n",
    "]\n",
    "\n",
    "# Polling settings for alerts-index\n",
    "ALERT_POLL_TIMEOUT_S = 300.0\n",
    "ALERT_POLL_INTERVAL_S = 1.0\n",
    "\n",
    "print(\"Loaded\", len(TEST_QUERIES), \"test queries\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(test_queries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    baseline_alert_ts = es_latest_alert_ts()\n",
    "    print(\"Baseline newest alert @timestamp:\", baseline_alert_ts)\n",
    "\n",
    "    with pg_connect() as conn:\n",
    "        with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:\n",
    "            for q in test_queries:\n",
    "                name = q[\"name\"]\n",
    "                sql = q[\"sql\"]\n",
    "\n",
    "                # Record pre-query alert timestamp (newest alert doc)\n",
    "                pre_alert_ts = es_latest_alert_ts()\n",
    "\n",
    "                # Run query and time it\n",
    "                t_start_iso = utc_now_iso()\n",
    "                t_start = time.time()\n",
    "                try:\n",
    "                    cur.execute(sql)\n",
    "                    # Fetch all result sets if any (only last SELECT is fetchable)\n",
    "                    row = None\n",
    "                    if cur.description is not None:\n",
    "                        row = cur.fetchone()\n",
    "                except Exception as e:\n",
    "                    row = None\n",
    "                    err = repr(e)\n",
    "                else:\n",
    "                    err = None\n",
    "                t_end = time.time()\n",
    "                t_end_iso = utc_now_iso()\n",
    "\n",
    "                # Wait for a new alert to appear (best-effort)\n",
    "                wait_result = es_wait_new_alert(pre_alert_ts, timeout_s=ALERT_POLL_TIMEOUT_S, poll_s=ALERT_POLL_INTERVAL_S)\n",
    "\n",
    "                results.append({\n",
    "                    \"name\": name,\n",
    "                    \"sql\": sql,\n",
    "                    \"query_start_utc\": t_start_iso,\n",
    "                    \"query_end_utc\": t_end_iso,\n",
    "                    \"query_duration_s\": round(t_end - t_start, 6),\n",
    "                    \"pg_result_first_row\": row,\n",
    "                    \"error\": err,\n",
    "                    \"alerts_newest_ts_before\": pre_alert_ts,\n",
    "                    **wait_result,\n",
    "                })\n",
    "\n",
    "                print(f\"[{name}] duration={results[-1]['query_duration_s']}s alert_status={wait_result['status']} wait={wait_result['wait_s']}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "eval_results = run_eval(TEST_QUERIES)\n",
    "print(\"Done. Rows:\", len(eval_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save results to CSV so you can analyze outside the notebook\n",
    "out_path = \"./logging_latency_eval.csv\"\n",
    "df = pd.DataFrame(eval_results)\n",
    "df.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Wrote\", out_path)\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
